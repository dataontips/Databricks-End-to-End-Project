# Azure Databricks End-to-End Data Engineering Project

## ğŸ“Œ Overview

This repository showcases an **end-to-end Data Engineering project built on Azure Databricks**, created as part of my personal portfolio to demonstrate a real-world implementation of the **Medallion Architecture (Bronze, Silver, Gold)** using modern data engineering best practices.

The project covers **full cloud setup**, **streaming ingestion using Auto Loader**, **batch/stream processing**, **job orchestration**, and **analytical fact table creation**, and is intended to be used as a **personal portfolio project** to demonstrate my hands-on experience to recruiters and hiring managers.

---

## ğŸ—ï¸ Architecture Overview

The solution follows a layered architecture based on the **Medallion Architecture** pattern:

**Source â†’ Bronze â†’ Silver â†’ Gold â†’ Fact Tables â†’ Power BI**

* **Bronze Layer**: Raw ingestion from ADLS Gen2 using Databricks Auto Loader
* **Silver Layer**: Data cleansing, standardization, and business entity separation
* **Gold Layer**: Business-ready dimensions, facts, and SCD handling
* **Power BI**: Reporting and analytics layer (in progress)

### ğŸ” Databricks Job Orchestration

The pipeline is orchestrated using **Databricks Jobs**, where each layer depends on the successful completion of the previous one.

![Databricks Pipeline Architecture](https://github.com/dataontips/Databricks-End-to-End-Project/blob/462676906cd612c576f80b4fef07b217c6388e44/Images/databricks_pipeline.png)

---

## ğŸ”§ Technologies Used

* **Cloud Platform**: Microsoft Azure
* **Data Processing**: Azure Databricks (PySpark)
* **Storage**: Azure Data Lake Storage Gen2 (ADLS)
* **Ingestion**: Databricks Auto Loader
* **Architecture Pattern**: Medallion Architecture
* **Orchestration**: Databricks Jobs
* **Metastore**: Unity Catalog (Metastore & External Locations)
* **Version Control**: GitHub

---

## â˜ï¸ Azure Resources Created

All resources were provisioned and configured manually as part of the project:

* Azure Resource Group
* Azure Storage Account (ADLS Gen2)

  * Containers for source, bronze, silver, and gold layers
* Azure Databricks Workspace
* Databricks Access Connector
* Unity Catalog Metastore
* External Locations & Storage Credentials

---

## ğŸ“‚ Project Structure

> The data files included in this repository are **sample datasets** used for demonstration and portfolio purposes only.

````text
Databricks-ETE-Project/
â”‚
â”œâ”€â”€ data/                  # Sample source data (Parquet)
â”‚   â”œâ”€â”€ customer_first.parquet
â”‚   â”œâ”€â”€ customers_second.parquet
â”‚   â”œâ”€â”€ orders_first.parquet
â”‚   â”œâ”€â”€ orders_second.parquet
â”‚   â”œâ”€â”€ products_first.parquet
â”‚   â”œâ”€â”€ products_second.parquet
â”‚   â””â”€â”€ regions.parquet
â”‚
â”œâ”€â”€ notebooks/             # Databricks notebooks (.dbc export)
â”‚   â””â”€â”€ Databricks_ETE_Project.dbc
â”‚
â”œâ”€â”€ images/                # Architecture & report screenshots
â”‚   â”œâ”€â”€ databricks_pipeline.png
â”‚   â””â”€â”€ powerbi_report.png
â”‚
â”œâ”€â”€ power_bi/              # Power BI report
â”‚   â””â”€â”€ sales_analytics.pbix   # (In Progress)
â”‚
â””â”€â”€ README.md

Databricks-ETE-Project/
â”‚
â”œâ”€â”€ bronze_layer/
â”‚   â””â”€â”€ bronze_autoloader.dbc
â”‚
â”œâ”€â”€ silver_layer/
â”‚   â”œâ”€â”€ silver_customers.dbc
â”‚   â”œâ”€â”€ silver_orders.dbc
â”‚   â””â”€â”€ silver_products.dbc
â”‚
â”œâ”€â”€ gold_layer/
â”‚   â”œâ”€â”€ gold_customers.dbc
â”‚   â”œâ”€â”€ gold_products_dlt.dbc
â”‚   â””â”€â”€ fact_orders.dbc
â”‚
â”œâ”€â”€ power_bi/
â”‚   â””â”€â”€ sales_analytics.pbix   # (In Progress)
â”‚
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ job_orchestration_diagram.png
â”‚
â””â”€â”€ README.md
````

---

## ğŸ”„ Data Pipeline Flow

---

## ğŸ§¬ Slowly Changing Dimensions (SCD)

This project implements **both SCD Type 1 and SCD Type 2** patterns in the **Gold layer**, demonstrating how dimensional data is handled in real-world data warehouses.

### ğŸ”¹ SCD Type 1 â€“ Overwrite History (Customers Dimension)

**Use case:** When historical changes are *not required* (e.g., correcting customer attributes).

**Implemented in:** `gold_customers.py`

**Approach:**

* Source data is read from the **Silver Customers** table
* Records are de-duplicated on `customer_id`
* Existing vs new records are identified using a **left join** against the Gold dimension table
* A **surrogate key (`DimCustomerKey`)** is generated for new records
* `create_date` and `update_date` are maintained
* Data is merged into the Gold table using **Delta Lake MERGE**

**Key Logic:**

* Existing records â†’ updated in place
* New records â†’ inserted
* No historical versions are preserved

This logic ensures the **latest version of each customer** is always available in the Gold layer.

---

### ğŸ”¹ SCD Type 2 â€“ Preserve History (Products Dimension)

**Use case:** When historical changes *must be tracked* (e.g., product price/category changes).

**Implemented in:** `gold_products.py` using **Delta Live Tables (DLT)**

**Approach:**

* Streaming data is read from the **Silver Products** table
* Data quality rules are enforced using **DLT expectations**
* A staging table and streaming view are created
* Changes are applied using `dlt.apply_changes()` with `stored_as_scd_type = 2`

**Key Features:**

* Maintains full history of product changes
* Automatically manages:

  * `effective_start_date`
  * `effective_end_date`
  * `is_current` flag
* Each change creates a **new version** of the record

This approach demonstrates a **production-grade SCD Type 2 implementation** using Databricks declarative pipelines.

---

## ğŸ”„ Data Pipeline Flow

### 1ï¸âƒ£ Bronze Layer â€“ Raw Ingestion

* Uses **Databricks Auto Loader** (`cloudFiles`)
* Ingests Parquet files from ADLS source container
* Supports schema evolution
* Checkpointing enabled for fault tolerance
* Parameterized using Databricks widgets

### 2ï¸âƒ£ Silver Layer â€“ Data Transformation

* Data cleansing and normalization
* Type casting and column standardization
* Business-level entity separation:

  * Customers
  * Orders
  * Products

### 3ï¸âƒ£ Gold Layer â€“ Business Aggregates

* Curated, analytics-ready tables
* Gold tables created using standard Spark jobs and DLT
* Optimized for downstream consumption

### 4ï¸âƒ£ Fact Tables

* Creation of `fact_orders` table
* Designed for BI and analytical use cases

---

## ğŸ§© Orchestration Strategy

* Implemented using **Databricks Jobs**
* Job dependencies reflect Medallion flow
* Separate jobs for Bronze, Silver, and Gold layers
* Supports reruns and failure recovery via checkpoints

---

## ğŸ” Governance & Security

* Unity Catalog enabled
* External locations configured for ADLS access
* Centralized metastore for table governance
* Separation of storage and compute

---

## ğŸ“Š Power BI Reporting (In Progress)

The Power BI report connects to curated **Gold layer tables** created in Databricks and is intended to support business reporting and analysis use cases.

![Power BI Report](images/powerbi_report.png)

---

## ğŸš€ Key Highlights

* End-to-end Azure Databricks implementation
* Production-style Medallion Architecture
* Streaming + batch hybrid processing
* Parameterized and reusable pipelines
* Job-based orchestration
* Portfolio-ready project structure

---

## ğŸ“ˆ Possible Enhancements

* Add data quality checks using Delta Live Tables expectations
* CI/CD integration using GitHub Actions
* Monitoring and alerting via Azure Monitor
* Power BI dashboard connected to Gold layer

---

## ğŸ‘¤ Author

**Navjeet Singh**
Data Engineer | Azure | Databricks

This repository is part of my personal portfolio to showcase practical, hands-on experience with Azure Databricks and modern data engineering workflows. If you find it useful, feel free to â­ the repository or connect with me.

---

## ğŸ“œ License

This project is for learning and portfolio purposes. You are free to fork and adapt it with attribution.
